# -*- coding: utf-8 -*-
"""recommenSystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUGDtL4ptDahvQPadOZemCghxhezg2Y2

##Import thư viện
"""

!pip install sentence_transformers

import matplotlib.pyplot as plt
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

"""#kết nối drive"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import os
colab_dir = '/content/gdrive/My Drive'
my_dir    = '/BIG DATA FINAL'
path      = colab_dir + my_dir

# %cd $path

"""#Crawl data"""

#connect notebook with dwh using SQLAlchemy
#load database
databases = json.loads('\
{\Driver={PostgreSQL Unicode(x64)};Server=18.142.59.122;Port=32768;Database=dwh;
   "bipbip": {\
      "structure": "postgresql://{username}:{password}@{host}:{port}/{database}",\
      "configs": {\
            "username": "",\
            "password": "",\
            "host": "",\
            "port": "",\
            "database": ""\
      }\
   }

#call a function: flow idea : write query from db to test first ->call function with purpose that can query right on notebook and connected direct to dwh, 
# idea grow: query by file instead of a string -> result sẽ được chứa trong dataframe
def query_from_dwh(queryString:str, server:str, replaceMent:pd.DataFrame = None):
   is_ok = False
   db_name = ''
   if server.lower() in ['dwh', 'bb', 'bipbip']:
      db_name = 'bipbip'
   else:
      is_ok = False
   if db_name == '':
      df = pd.DataFrame({
         'result':["cant load database"],
         'type': ['something wrong with database name'],
         'value':["There is no data base like that"]
      })
   else:
      dbstructure = databases[db_name]["structure"]
      dbconfigs = databases[db_name]["configs"]
      engineString = dbstructure
      for ele in dbconfigs:
         findText = "{" + ele + "}"
         replace_to = dbconfigs[ele]
         engineString = engineString.replace(findText,replace_to)
   if replaceMent is not None:
      for i in range(0,replaceMent.shape[0]):
         queryString = queryString.replace(replaceMent.iloc[i,0],replaceMent.iloc[i,1])
   queryString = queryString.replace('%','%%')
   try:
      df = pd.read_sql_query(queryString, con=engineString)
      is_ok = True
      message = 'successfully'
   except BaseException as e:
      err_type, err_value, track_tb = read_error()
      df = pd.DataFrame({
         'result':["error"],
         'type': [err_type],
         'value':[err_value]
      })
      is_ok = False
      message = "fail: " + err_value
   return is_ok, df, message

def query_and_send(Query_FilePath:str, message:str):
   is_ok, df, message = fx_query.query_from_dwh(queryString, "bb")
   print(df)

   if is_ok:
      output_filepath = f'view_dataset.csv'
      # df.sort_values(by=['created_at'], ascending=False)
      df.to_csv(output_filepath, index=False)
      print(f'Exported {output_filepath}')
   else:
      err_mess = "error when query file"
      print(message)
   
   if is_ok:
      print("Done")
   else:
      print(err_mess)

"""#Import data"""

df= pd.read_csv('view_dataset.csv', encoding='utf-8')

df.head()

df.info()

df.type.unique()

"""#EDA"""

plt.rcParams["figure.figsize"] = (30,20)

view_cat_count= pd.DataFrame(df.groupby('type')['customer_id'].count().reset_index())
view_cat_count.columns = ['type','count']
plt.bar(x=view_cat_count['type'], height = view_cat_count['count'] )

view_count= pd.DataFrame(data_CF.groupby('view_count')['customer_id'].count().reset_index())
view_count.columns = ['no_view','count']
view_count = view_count.sort_values(by = 'count', ascending= False).head(20)
fig1, ax1 = plt.subplots()
ax1.pie(view_count['count'], labels=view_count['no_view'], autopct='%1.1f%%',
        shadow=True, startangle=90)

brand_count = pd.DataFrame(df.groupby('brand')['customer_id'].count().reset_index())
brand_count.columns = ['brand', 'count']
brand_top10 = brand_count.nlargest(6, 'count')  # lấy 5 thương hiệu có số lượt xem hàng lớn nhất
plt.bar(x=brand_top10['brand'], height=brand_top10['count'])
plt.show()

"""#Collaborative Filtering"""

### pre-processing

import nltk 
from nltk.stem import SnowballStemmer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
import re
import tensorflow as tf
print("Tensorflow Version",tf.__version__)

import warnings 
warnings.filterwarnings('ignore')

data_CF = df.groupby(by = ['customer_id','sku_id'])['view_date'].count().reset_index()

data_CF = data_CF.rename(columns = {'view_date':'view_count'})

display(data_CF)

data_CF = data_CF.drop_duplicates()

count = data_CF.copy()

display(count)

"""##skuid - name"""

productid = df.groupby(by = ['sku_id','prod_names'])['view_date'].count().reset_index().drop(columns = ['view_date'])
productid = productid.sort_values(by = ['sku_id']).reset_index().drop(columns = ['index'])

display(productid)

"""## product sku_id"""

product_list = pd.DataFrame(sorted(count.sku_id.unique().tolist())).reset_index()#.drop(columns='index')

display(product_list)

product_list = product_list.rename(columns = {0: 'sku_id'})

prod_dict = dict(product_list.values)

encoding_dict = dict([(value, key) for key, value in prod_dict.items()])

count.sku_id = count.sku_id.map(encoding_dict)

"""## customer id"""

cust = pd.DataFrame(sorted(count.customer_id.unique().tolist())).reset_index()#.drop(columns='index')

display(cust)

cust_dict = dict(cust.values)

cust_encoding = dict([(value, key) for key, value in cust_dict.items()])

count.customer_id = count.customer_id.map(cust_encoding)

cust = cust.rename(columns = {0: 'customer_id'})

"""##user-item matrix"""

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns

row = np.array(count.sku_id.tolist())
col = np.array(count.customer_id.tolist())
data = np.array(count.view_count.tolist())
csr_sample = csr_matrix((data, (row, col)), shape=(len(count.sku_id.unique().tolist()), len(count.customer_id.unique().tolist())))

"""##cosine similarity"""

knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=3, n_jobs=-1)
knn.fit(csr_sample)

"""## build a CF algo"""

def item_viewed(cust_id, count):
  filter1 = count[count['customer_id'] == cust_id].sku_id
  filter1 = filter1.tolist()
  filter1 = filter1[:20]
  return filter1

def rec_list(item_viewed):
  distances1=[]
  indices1=[]
  for i in item_viewed:
    distances , indices = knn.kneighbors(csr_sample[i],n_neighbors=10)
    indices = indices.flatten()
    indices= indices[1:]
    indices1.extend(indices)
  return [x for x in list(dict.fromkeys(indices1)) if x not in item_viewed][:20]

#def CF(cust_list, product_list): 
def ask():
  start = input('\nYou ask for a recommendation list? (Y / N)\n-> ').upper()
  return start

def print_list(list_id, table_prod):
    rec_list = []
    x=1
    for i in list_id:
      if (i<len(table_prod)):
        print(str(x),'. ', table_prod.loc[table_prod.index == i, 'prod_names'].item())
        rec_list.append(table_prod.loc[table_prod.index == i, 'prod_names'].item())
      x+=1
    return rec_list


while (ask() == 'Y'):
  or_id = int(input( '\nInsert the customer ID ordinal number: '))
  if (or_id>len(cust)):
    print('Your ID has not been created. Try again.')
  else:
    print('Customer ID:', cust.loc[cust['index'] == or_id, 'customer_id'].squeeze())
    hist_viewed = item_viewed(or_id, count)

    rec_prod_id = rec_list(item_viewed(or_id, count)) #list mã(encoding/index) các sản phẩm gợi ý
    
    print('\nViewed products:\n')
    history_list = print_list(hist_viewed, productid)
    
    print('\nProducts you may like:\n')
    rec_prod_skuid = print_list(rec_prod_id, productid) #list tên các sản phẩm gợi ý
    
  ask

print(rec_prod_id)

print(hist_viewed)

"""#Content-based Filtering

##preprocessing
"""

data_CB=pd.merge(product_list,df[['descrip','prod_names']], left_on='prod_name', right_on='prod_names' ,how='left')

data_CB.head()

data_CB_copy = data_CB[['index','prod_name','descrip']]

display(data_CB_copy)

data_CB_copy.info()

data_CB_copy=data_CB_copy.dropna()
data_CB_copy=data_CB_copy[['index','prod_name','descrip']].drop_duplicates()

data_CB_copy.info()

X = np.array(data_CB_copy.descrip)

"""##BERT"""

#text_data = X

text_data = X
model = SentenceTransformer('distilbert-base-nli-mean-tokens')
embeddings = model.encode(text_data, show_progress_bar=True)

type(embeddings)

embedded= embeddings

np.savetxt('bertmatrix.txt', embedded, delimiter='\t')

print(embeddings)

"""##CB algo"""

Y = np.array(embedded)
cos_sim_data = pd.DataFrame(cosine_similarity(Y))

def give_recommendations(index, print_recommendation=False, print_recommendation_plots=False, print_genres=False):
    index_recomm = cos_sim_data.loc[index].sort_values(ascending=False).index.tolist()[1:6]
    sku_recomm = product_list['sku_id'].loc[index_recomm].values
    result = {'sku_id': sku_recomm, 'Index': index_recomm}
    
#     if print_recommendation:
#         print('The watched product is this one: %s \n' % (df['prod_names'].loc[index]))
#         for i, prod in enumerate(prods_recomm):
#             print('The number %i recommended product is this one: %s \n' % (i + 1, prod))

    if print_recommendation:
        print('The watched product is this one: %s (sku_id: %s) \n' % (productid['prod_names'].loc[index], productid['sku_id'].loc[index]))
        for i, sku in enumerate(sku_recomm):
            print('The number %i recommended product is this one: %s (sku_id: %s) \n' % (i + 1, productid[productid.sku_id == sku]['prod_names'].squeeze(), sku))
            
    if print_recommendation_plots:
        print('The plot of the watched product is this one:\n %s \n' % (df['descrip'].loc[index]))
        for i, q in enumerate(range(len(sku_recomm))):
            plot_q = df['descrip'].loc[index_recomm[q]]
            print('The plot of the number %i recommended product is this one:\n %s \n' % (i + 1, plot_q))
            
    if print_genres:
        print('The genres of the watched product is this one:\n %s \n' % (df['type'].loc[index]))
        for i, q in enumerate(range(len(sku_recomm))):
            genre_q = df['type'].loc[index_recomm[q]]
            print('The genre of the number %i recommended product is this one:\n %s \n' % (i + 1, genre_q))
            
    return result

for i in rec_prod_id:
    print('>> NAME OF TOP 5 PRODUCT RECOMMENDATION')
    give_recommendations(i,True)

while (ask() == 'Y'):
  or_id = int(input( '\nInsert the customer ID ordinal number: '))
  if (or_id>len(cust)):
    print('Your ID has not been created. Try again.')
  else:
    print('Customer ID:', cust.loc[cust['index'] == or_id, 'customer_id'].squeeze())
    hist_viewed = item_viewed(or_id, count)

    rec_prod_id = rec_list(item_viewed(or_id, count)) #list mã(encoding/index) các sản phẩm gợi ý
    
    print('\nViewed products:\n')
    history_list = print_list(hist_viewed, productid)
    
    print('\nProducts you may like:\n')
    rec_prod_skuid = print_list(rec_prod_id, productid) #list tên các sản phẩm gợi ý
    
    for i in rec_prod_id:
      print('\n>> NAME OF TOP 5 PRODUCT RECOMMENDATION')
      give_recommendations(i,True)

  ask

"""#RESULT ANALYSIS"""

data_CF[data_CF.customer_id == cust.loc[cust['index'] == or_id, 'customer_id'].squeeze()].merge(df)[['customer_id', 'prod_names', 'view_count', 'normal_price', 'type']]

history = (pd.DataFrame(history_list, columns = ['prod_names']).merge(data_CF)).merge(df)[['customer_id', 'prod_names', 'view_count', 'normal_price', 'type', 'brand']]
history = history[history.customer_id == cust.loc[cust['index'] == or_id, 'customer_id'].squeeze()]
display(history)

rec_df = pd.DataFrame(rec_prod_name, columns = ['prod_names']).merge(df)[['prod_names', 'customer_id', 'sku_id', 'brand', 'normal_price', 'type']]#.drop_duplicates()
display(rec_df)#[rec_df.customer_id == cust.loc[cust['index'] == or_id, 'customer_id'].squeeze()])

import math
import matplotlib.pyplot as plt
import seaborn           as sbn
import plotly.express    as px
import statsmodels.api as sm
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import gmean, hmean, iqr, skew, spearmanr, pearsonr, entropy, levene
from IPython.display import Image

fig = plt.figure(figsize = ([12, 8])) # xác dịnh kích thước khung (width, height): inch

plt.subplot(1, 2, 1)
type_table1 = history.groupby(by = 'type')['view_count'].count().reset_index()#.drop(columns = 'index')
plt.pie(type_table1['view_count'], labels = type_table1['type'], autopct='%1.1f%%', counterclock = False)
plt.title('Proportion of type (viewed)')

plt.subplot(1, 2, 2)
type_table2 = rec_df.groupby(by = 'type')['sku_id'].count().reset_index()#.drop(columns = 'index')
plt.pie(type_table2['sku_id'], labels = type_table2['type'], autopct='%1.1f%%', counterclock = False)
plt.title('Proportion of type (recommended)')

plt.show()

fig = plt.figure(figsize = ([12, 15])) # xác dịnh kích thước khung (width, height): inch

plt.subplot(2, 1, 1)
brand_table1 = history.groupby(by = 'brand')['view_count'].count().reset_index()#.drop(columns = 'index')
plt.barh(history['brand'], history['view_count'])
plt.title('Brand (viewed)')

plt.subplot(2, 1, 2)
brand_table2 = rec_df.groupby(by = 'brand')['sku_id'].count().reset_index()#.drop(columns = 'index')
plt.barh(brand_table2['brand'], brand_table2['sku_id'])
plt.title('Brand (recommended)')

plt.show()

fig = plt.figure(figsize = ([12, 15])) # xác dịnh kích thước khung (width, height): inch

plt.subplot(2, 1, 1)
#type_table1 = history.groupby(by = 'type')['view_count'].count().reset_index()#.drop(columns = 'index')
sbn.boxplot(y=history['normal_price'], color = 'green')
#plt.plot(type_table1['view_count'], labels = type_table1['type'], autopct='%1.1f%%', counterclock = False)
plt.title('Normal price (viewed)')

plt.subplot(2, 1, 2)
#type_table2 = rec_df.groupby(by = 'type')['sku_id'].count().reset_index()#.drop(columns = 'index')
#plt.pie(type_table2['sku_id'], labels = type_table2['type'], autopct='%1.1f%%', counterclock = False)
sbn.boxplot(y=rec_df['normal_price'], color = 'orange')
plt.title('Normal price (recommended)')

plt.show()

"""# Ranking Metrics







"""

true_prod_id = hist_viewed
"""
# Tính MRR
def reciprocal_rank(ranks):
    ""Tính reciprocal rank của một danh sách xếp hạng.""
    for i, rank in enumerate(ranks):
        if rank:
            return 1.0 / (i + 1)
    return 0.0

def mean_reciprocal_rank(recommendations, true_ranks):
    ""Tính trung bình reciprocal rank của các người dùng.""
    rr_sum = 0.0
    for ranks, true_rank in zip(recommendations, true_ranks):
        rr = reciprocal_rank([r == true_rank for r in ranks])
        rr_sum += rr
    return rr_sum / len(recommendations)

recommendations = [rec_prod_id] # danh sách sản phẩm được đề xuất bởi hệ thống gợi ý
true_ranks = [[p in true_prod_id for p in rec_prod_id]] # danh sách đánh dấu các sản phẩm thực tế được đề xuất

mrr = mean_reciprocal_rank(recommendations, true_ranks)
print("MRR:", mrr)
"""
# Tính NDCG
import numpy as np

def dcg_at_k(r, k):
    """Tính DCG@k của một danh sách xếp hạng r."""
    r = np.asfarray(r)[:k]
    if r.size:
        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))
    return 0.0

def ndcg_at_k(r, k):
    """Tính nDCG@k của một danh sách xếp hạng r."""
    dcg_max = dcg_at_k(sorted(r, reverse=True), k)
    if not dcg_max:
        return 0.0
    return dcg_at_k(r, k) / dcg_max

def average_ndcg(true_ranks, k):
    """Tính trung bình nDCG@k của các người dùng."""
    ndcg_sum = 0.0
    for ranks in true_ranks:
        ndcg = ndcg_at_k(ranks, k)
        ndcg_sum += ndcg
    return ndcg_sum / len(true_ranks)

k = len(rec_prod_id) # số lượng sản phẩm được đề xuất bởi hệ thống
ndcg = average_ndcg(true_ranks, k)
print("nDCG@{}:".format(k), ndcg)